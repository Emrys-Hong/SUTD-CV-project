{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "siAmEc3oonfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "774a4bbe-2291-4832-a36d-6c835aabc066"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar  4 17:40:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-MMPbIuNMAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4cdd2385-c770-42b7-ccf1-5c0ca65548f8"
      },
      "source": [
        "!wget -c \"https://sutdapac-my.sharepoint.com/:u:/g/personal/gary_ong_mymail_sutd_edu_sg/EayDZjbTF3JAi1VTlyiU5skB5VLEdUjjFTrktsTm29WB_Q?e=aO1YJR&download=1\" -O 701515.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 17:40:41--  https://sutdapac-my.sharepoint.com/:u:/g/personal/gary_ong_mymail_sutd_edu_sg/EayDZjbTF3JAi1VTlyiU5skB5VLEdUjjFTrktsTm29WB_Q?e=aO1YJR&download=1\n",
            "Resolving sutdapac-my.sharepoint.com (sutdapac-my.sharepoint.com)... 13.107.136.9\n",
            "Connecting to sutdapac-my.sharepoint.com (sutdapac-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/gary_ong_mymail_sutd_edu_sg/Documents/BengaliDataset/701515.zip?&originalPath=aHR0cHM6Ly9zdXRkYXBhYy1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9nYXJ5X29uZ19teW1haWxfc3V0ZF9lZHVfc2cvRWF5RFpqYlRGM0pBaTFWVGx5aVU1c2tCNVZMRWRVampGVHJrdHNUbTI5V0JfUT9ydGltZT1jMVhUTEdQQTEwZw [following]\n",
            "--2020-03-04 17:40:48--  https://sutdapac-my.sharepoint.com/personal/gary_ong_mymail_sutd_edu_sg/Documents/BengaliDataset/701515.zip?&originalPath=aHR0cHM6Ly9zdXRkYXBhYy1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9nYXJ5X29uZ19teW1haWxfc3V0ZF9lZHVfc2cvRWF5RFpqYlRGM0pBaTFWVGx5aVU1c2tCNVZMRWRVampGVHJrdHNUbTI5V0JfUT9ydGltZT1jMVhUTEdQQTEwZw\n",
            "Reusing existing connection to sutdapac-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525343594 (501M) [application/x-zip-compressed]\n",
            "Saving to: ‘701515.zip’\n",
            "\n",
            "701515.zip          100%[===================>] 501.01M  57.5MB/s    in 12s     \n",
            "\n",
            "2020-03-04 17:41:01 (41.6 MB/s) - ‘701515.zip’ saved [525343594/525343594]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unYcjx-sNhqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "dbe0a6d3-0c8c-4bb4-d3f1-1c1ceb1dc4cc"
      },
      "source": [
        "!unzip 701515.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  701515.zip\n",
            "  inflating: s_test_label2.csv       \n",
            "  inflating: s_test2.npy             \n",
            "  inflating: s_train_label2.csv      \n",
            "  inflating: s_train2.npy            \n",
            "  inflating: s_val_label2.csv        \n",
            "  inflating: s_val2.npy              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4NVJMYqRr8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2f51983c-b734-4daf-fba2-9c9035f92ca0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "701515.zip   s_test2.npy\ts_train2.npy\t    s_val2.npy\n",
            "sample_data  s_test_label2.csv\ts_train_label2.csv  s_val_label2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHTVSya3SCne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "74bf5832-7be4-4de4-edb1-cfa159e2dd2a"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-ux8zm1yv\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-ux8zm1yv\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (6.2.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (3.1.3)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (45.2.0)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.4-cp36-none-any.whl size=64354 sha256=ce212f31f575ba27aabe76c968c9a63543162053ba2a03a6a177910bd54817e2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n1bt26ec/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=7611f7b5e93b60c064fdc6495ba1aa97e07ab034a3a7e3e73eeaf9acf323cc7b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n1bt26ec/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.4 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaHtOHNYNqHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "class BengaliDataset2(Dataset):\n",
        "    def __init__(self,npy_file,label_csv,aug=None,norm=None):\n",
        "        self.npy_file = np.load(npy_file)\n",
        "        self.norm = norm\n",
        "        df = pd.read_csv(label_csv)\n",
        "        # for faster access i think\n",
        "        self.grapheme_root = df[\"grapheme_root\"].values\n",
        "        self.vowel_diacritic = df[\"vowel_diacritic\"].values\n",
        "        self.consonant_diacritic = df[\"consonant_diacritic\"].values\n",
        "\n",
        "        self.aug = aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_arr = self.npy_file[index]\n",
        "        # only do this on training\n",
        "        #use albumentations library\n",
        "        if self.aug != None:\n",
        "            image_arr = self.aug(image=image_arr)[\"image\"]\n",
        "\n",
        "        image_arr = torch.from_numpy(image_arr)\n",
        "        image_arr = image_arr/255.0 #normalize and converts to float32\n",
        "\n",
        "        if self.norm != None:\n",
        "            mean = self.norm['mean']\n",
        "            std = self.norm['std']\n",
        "            image_arr = (image_arr -  mean)/std\n",
        "\n",
        "        grapheme_root = torch.Tensor([self.grapheme_root[index]]).long()\n",
        "        vowel_diacritic = torch.Tensor([self.vowel_diacritic[index]]).long()\n",
        "        consonant_diacritic = torch.Tensor([self.consonant_diacritic[index]]).long()\n",
        "        \n",
        "        return {\"image\":image_arr.unsqueeze(0),\"grapheme_root\":grapheme_root,\"vowel_diacritic\":vowel_diacritic,\"consonant_diacritic\":consonant_diacritic}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.npy_file.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6zZuGQeRZ1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import albumentations as A\n",
        "import torchvision.transforms as transforms\n",
        "mean = 16.922\n",
        "std = 51.398\n",
        "\n",
        "shift_scale_rotate = A.augmentations.transforms.ShiftScaleRotate(p=0.5,scale_limit=0.4,rotate_limit=30)\n",
        "brightness = A.augmentations.transforms.RandomBrightness(p=0.5)\n",
        "# grid_distortion = A.augmentations.transforms.GridDistortion(p=0.5,distort_limit=0.4)\n",
        "# blur = A.augmentations.transforms.Blur(p=0.2)\n",
        "# opticalDist = A.augmentations.transforms.OpticalDistortion(p=0.5)\n",
        "# elasticTransform = A.augmentations.transforms.ElasticTransform(p=0.5,alpha_affine=10)\n",
        "# gd = A.augmentations.transforms.GridDropout(unit_size_min=20,unit_size_max=100,p=0.5)\n",
        "aug_list = [shift_scale_rotate,brightness]\n",
        "augment = A.core.composition.Compose(aug_list,p=0.5)\n",
        "augment=None\n",
        "train_data = BengaliDataset2(\"s_train2.npy\",\"s_train_label2.csv\",aug =augment,norm={'mean':mean,'std':std})\n",
        "val_data = BengaliDataset2(\"s_val2.npy\",\"s_val_label2.csv\",norm={'mean':mean,'std':std})\n",
        "test_data = BengaliDataset2(\"s_test2.npy\",\"s_test_label2.csv\",norm={'mean':mean,'std':std})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmJURapYS0k-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, criterion, optimizer, device, dataloaders, scheduler=None, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    dataset_sizes = {'train': len(dataloaders['train'].dataset),'val': len(dataloaders['val'].dataset),'test':len(dataloaders['test'].dataset)}\n",
        "\n",
        "    train_acc_list = []; train_loss_list= []; val_acc_list = []; val_loss_list = []; test_acc_list = []; test_loss_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val','test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            grapheme_corrects = 0\n",
        "            vowel_corrects = 0\n",
        "            consonant_corrects = 0\n",
        "\n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "\n",
        "                inputs = data['image']\n",
        "\n",
        "                grapheme_root_label = data['grapheme_root']\n",
        "                vowel_diacritic_label = data['vowel_diacritic']\n",
        "                consonant_diacritic_label = data['consonant_diacritic']\n",
        "                \n",
        "         \n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                grapheme_root_label =  grapheme_root_label.to(device)\n",
        "                vowel_diacritic_label = vowel_diacritic_label.to(device)\n",
        "                consonant_diacritic_label =  consonant_diacritic_label.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    \n",
        "                    grapheme_preds = outputs[:,:168].argmax(dim=1)\n",
        "                    vowel_preds = outputs[:,168:179].argmax(dim=1) \n",
        "                    consonant_preds = outputs[:,179:186].argmax(dim=1)\n",
        "\n",
        "                    loss = criterion(outputs, grapheme_root_label.squeeze(1),vowel_diacritic_label.squeeze(1),consonant_diacritic_label.squeeze(1))\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                  \n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                \n",
        "\n",
        "                grapheme_corrects += torch.sum(grapheme_preds == grapheme_root_label.data.squeeze(1))\n",
        "                vowel_corrects += torch.sum(vowel_preds == vowel_diacritic_label.data.squeeze(1))\n",
        "                consonant_corrects += torch.sum(consonant_preds== consonant_diacritic_label.data.squeeze(1))\n",
        "                \n",
        "        \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            running_corrects = 0.5*grapheme_corrects.double() + 0.25*vowel_corrects.double() + 0.25*consonant_corrects.double()\n",
        "\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "            if phase == 'val' and scheduler != None:\n",
        "                scheduler.step(epoch_loss)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == \"train\":\n",
        "                # Note this are running values (calculated per batch) rather than actual values at the end of each epoch\n",
        "                # Decreases training time\n",
        "                # Not accurate especially at first few epochs\n",
        "                train_acc_list.append(epoch_acc)\n",
        "                train_loss_list.append(epoch_loss)\n",
        "            elif phase == \"val\":\n",
        "                val_acc_list.append(epoch_acc)\n",
        "                val_loss_list.append(epoch_loss)\n",
        "            elif phase == \"test\":\n",
        "                test_acc_list.append(epoch_acc)\n",
        "                test_loss_list.append(epoch_loss)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        end = time.time()\n",
        "        print(f\"time per epoch:{end-start}s\")\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    plots = (train_acc_list,train_loss_list,val_acc_list,val_loss_list,test_acc_list,test_loss_list)\n",
        "\n",
        "    return model, plots\n",
        "\n",
        "def plot_model_metrics(plots,name):\n",
        "    train_acc_list,train_loss_list,val_acc_list,val_loss_list,test_acc_list,test_loss_list = plots\n",
        "    plot(train_acc_list,val_acc_list,test_acc_list,\"accuracy\",name)\n",
        "    plot(train_loss_list,val_loss_list,test_loss_list,\"loss\",name)\n",
        "\n",
        "\n",
        "def plot(train,val,test,metric,name):\n",
        "    plt.title(name)\n",
        "    plt.plot(train,label=\"train {}\".format(metric))\n",
        "    plt.plot(val,label=\"val {}\".format(metric))\n",
        "    plt.plot(test,label=\"test {}\".format(metric))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig(\"{}-{}\".format(name,metric))\n",
        "    plt.close()\n",
        "    \n",
        "def loss(outputs,grapheme_root_label,vowel_diacritic_label,consonant_diacritic_label):\n",
        "\n",
        "    grapheme_root_output = outputs[:,:168]\n",
        "    vowel_diacritic_output = outputs[:,168:179]\n",
        "    consonant_diacritic_output = outputs[:,179:186]\n",
        "    gloss = nn.CrossEntropyLoss()(grapheme_root_output,grapheme_root_label)\n",
        "    vloss = nn.CrossEntropyLoss()(vowel_diacritic_output,vowel_diacritic_label)\n",
        "    closs = nn.CrossEntropyLoss()(consonant_diacritic_output,consonant_diacritic_label)\n",
        "\n",
        "    return 0.5*gloss + 0.25*vloss + 0.25*closs\n",
        "\n",
        "def evaluate_test(model,criterion,dataloader,device):\n",
        "    running_loss = 0.0\n",
        "    grapheme_corrects = 0.0\n",
        "    vowel_corrects = 0.0\n",
        "    consonant_corrects = 0.0\n",
        "    grapheme_output = []\n",
        "    vowel_output = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "\n",
        "        inputs = data['image']\n",
        "\n",
        "        grapheme_root_label = data['grapheme_root']\n",
        "        vowel_diacritic_label = data['vowel_diacritic']\n",
        "        consonant_diacritic_label = data['consonant_diacritic']\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        grapheme_root_label =  grapheme_root_label.to(device)\n",
        "        vowel_diacritic_label = vowel_diacritic_label.to(device)\n",
        "        consonant_diacritic_label =  consonant_diacritic_label.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            grapheme_preds = outputs[:,:168].argmax(dim=1)\n",
        "            vowel_preds = outputs[:,168:179].argmax(dim=1)\n",
        "            consonant_preds = outputs[:,179:186].argmax(dim=1)\n",
        "\n",
        "            loss = criterion(outputs, grapheme_root_label.squeeze(1),vowel_diacritic_label.squeeze(1),consonant_diacritic_label.squeeze(1))\n",
        "\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        \n",
        "\n",
        "        grapheme_corrects += torch.sum(grapheme_preds == grapheme_root_label.data.squeeze(1))\n",
        "        vowel_corrects += torch.sum(vowel_preds == vowel_diacritic_label.data.squeeze(1))\n",
        "        consonant_corrects += torch.sum(consonant_preds== consonant_diacritic_label.data.squeeze(1))\n",
        "        \n",
        "    \n",
        "    loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    running_corrects = 0.5*grapheme_corrects.double() + 0.25*vowel_corrects.double() + 0.25*consonant_corrects.double()\n",
        "\n",
        "    # epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "    acc = running_corrects / len(dataloader.dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "        \"Final Test Accuracy\", loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrr1IfwTVGSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = DataLoader(train_data, batch_size=64, num_workers=4,shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=64, num_workers=4)\n",
        "\n",
        "dataloaders = {'train': train_loader,'val': val_loader,'test':test_loader}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANlkaYkwcWTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7573d464-bdac-46b6-9cd9-ecba43ef84df"
      },
      "source": [
        "# train all layers with pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "cn1 = nn.Parameter(torch.mean(model.conv1.weight, dim=1, keepdim=True))\n",
        "print(cn1[0])\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "model.conv1.weight = cn1"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0079, -0.0140, -0.0051,  0.0671,  0.0594,  0.0171, -0.0195],\n",
            "         [ 0.0241,  0.0082, -0.1135, -0.2950, -0.2791, -0.1398, -0.0085],\n",
            "         [ 0.0007,  0.0689,  0.3049,  0.5959,  0.5254,  0.2432,  0.0688],\n",
            "         [ 0.0170, -0.0739, -0.3054, -0.4331, -0.2530,  0.0119,  0.0800],\n",
            "         [-0.0373,  0.0097,  0.0661, -0.0481, -0.3040, -0.4163, -0.2593],\n",
            "         [ 0.0281,  0.0313,  0.0416,  0.2049,  0.4005,  0.3733,  0.1609],\n",
            "         [-0.0026,  0.0013, -0.0171, -0.0636, -0.1492, -0.0923, -0.0145]]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFzEpuq1xkmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1fb0349c-68f9-49e2-b957-ff2f3dbee5a9"
      },
      "source": [
        "print(model.conv1.weight[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0079, -0.0140, -0.0051,  0.0671,  0.0594,  0.0171, -0.0195],\n",
            "         [ 0.0241,  0.0082, -0.1135, -0.2950, -0.2791, -0.1398, -0.0085],\n",
            "         [ 0.0007,  0.0689,  0.3049,  0.5959,  0.5254,  0.2432,  0.0688],\n",
            "         [ 0.0170, -0.0739, -0.3054, -0.4331, -0.2530,  0.0119,  0.0800],\n",
            "         [-0.0373,  0.0097,  0.0661, -0.0481, -0.3040, -0.4163, -0.2593],\n",
            "         [ 0.0281,  0.0313,  0.0416,  0.2049,  0.4005,  0.3733,  0.1609],\n",
            "         [-0.0026,  0.0013, -0.0171, -0.0636, -0.1492, -0.0923, -0.0145]]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBsCIHZroSEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "face334b-2a27-4303-aba8-44b5b3842d8f"
      },
      "source": [
        "\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = loss\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=5,factor=0.3,verbose=True)\n",
        "\n",
        "model,plots = train_model(model, criterion, optimizer,\n",
        "            device, dataloaders,scheduler=scheduler, num_epochs=30)\n",
        "\n",
        "evaluate_test(model,criterion,test_loader,device)\n",
        "# plot_model_metrics(plots,\"graph\")\n",
        "\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.7543 Acc: 0.7998\n",
            "val Loss: 0.3195 Acc: 0.9075\n",
            "test Loss: 0.3248 Acc: 0.9061\n",
            "time per epoch:122.94647932052612s\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.2378 Acc: 0.9303\n",
            "val Loss: 0.2560 Acc: 0.9252\n",
            "test Loss: 0.2624 Acc: 0.9245\n",
            "time per epoch:123.3808844089508s\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.1572 Acc: 0.9531\n",
            "val Loss: 0.3322 Acc: 0.8976\n",
            "test Loss: 0.3421 Acc: 0.8951\n",
            "time per epoch:122.63356041908264s\n",
            "Epoch 4/30\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYrzE8plp6bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.Tensor([[1,2],[3,4]]).unsqueeze(0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ydXuT7qu--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}