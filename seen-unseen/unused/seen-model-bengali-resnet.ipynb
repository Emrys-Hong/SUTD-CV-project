{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import albumentations\n",
    "from efficientnet_pytorch import model as enet\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "# import apex;\n",
    "# from apex import amp\n",
    "from torchvision import models\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/xray/lib/python3.8/site-packages/sklearn/model_selection/_split.py:292: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1295' class='' max='1295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1295/1295 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel_type = 'resnet50-seen'\n",
    "enet_type = 'resnet50'\n",
    "fold = 0\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "image_size = 128\n",
    "cut_size = int(image_size * 0.85)\n",
    "data_dir = 'bengali'\n",
    "batch_size = 128\n",
    "num_workers = 32\n",
    "init_lr = 0.001\n",
    "c0_dim = 1295\n",
    "c1_dim = 168\n",
    "c2_dim = 11\n",
    "c3_dim = 7\n",
    "out_dim = c0_dim + c1_dim + c2_dim + c3_dim\n",
    "loss_weight = [4, 2, 1, 1, 2]\n",
    "n_epochs = 60\n",
    "use_amp = False\n",
    "\n",
    "files_train = [f'bengaliai-cv19-feather/train_image_data_{fid}.feather' for fid in range(4)]\n",
    "files_test = [f'bengaliai-cv19-feather/test_image_data_{fid}.feather' for fid in range(4)]\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(data_dir, f'train.csv'))\n",
    "df_class_map = pd.read_csv(os.path.join(data_dir, f'class_map.csv'))\n",
    "\n",
    "id2grapheme = {i: grapheme for i, grapheme in enumerate(df_train.grapheme.unique())}\n",
    "grapheme2id = {grapheme: i for i, grapheme in enumerate(df_train.grapheme.unique())}\n",
    "df_train['grapheme_id'] = df_train['grapheme'].map(grapheme2id)\n",
    "\n",
    "n_fold = 5\n",
    "skf = StratifiedKFold(n_fold, random_state=42)\n",
    "for i_fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train.grapheme)):\n",
    "    df_train.loc[val_idx, 'fold'] = i_fold\n",
    "df_train['fold'] = df_train['fold'].astype(int)\n",
    "\n",
    "df_label_map = []\n",
    "for i, df in progress_bar(df_train.groupby('grapheme_id')):\n",
    "    df_label_map.append(df.iloc[:, 1:6].drop_duplicates())\n",
    "df_label_map = pd.concat(df_label_map).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(files):\n",
    "    tmp = []\n",
    "    for f in files:\n",
    "        F = os.path.join(data_dir, f)\n",
    "        data = pd.read_feather(F)\n",
    "        res = data.iloc[:, 1:].values\n",
    "        imgs = []\n",
    "        for i in progress_bar(range(res.shape[0])):\n",
    "            img = res[i].squeeze().reshape(HEIGHT, WIDTH)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            imgs.append(img)\n",
    "        imgs = np.asarray(imgs)\n",
    "        \n",
    "        tmp.append(imgs)\n",
    "    tmp = np.concatenate(tmp, 0)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, csv, data, idx, split, mode, transform=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.data = data\n",
    "        self.idx = np.asarray(idx)\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.idx.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.idx[index]\n",
    "        this_img_id = self.csv.iloc[index].image_id\n",
    "        \n",
    "        image = self.data[index]\n",
    "        image = 255 - image\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image_origin = image.astype(np.float32).copy()\n",
    "            res = self.transform(image=image)\n",
    "            image = res['image'].astype(np.float32)\n",
    "        else:\n",
    "            image_origin = image.astype(np.float32).copy()\n",
    "            image = image.astype(np.float32)\n",
    "\n",
    "        image /= 255\n",
    "        image = image[np.newaxis, :, :]\n",
    "        image = np.repeat(image, 3, 0)  # 1ch to 3ch\n",
    "        ###\n",
    "        image_origin /= 255\n",
    "        image_origin = image_origin[np.newaxis, :, :]\n",
    "        image_origin = np.repeat(image_origin, 3, 0)  # 1ch to 3ch\n",
    "        ###\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return torch.tensor(image)\n",
    "        else:\n",
    "            label_0 = self.csv.iloc[index].grapheme_id\n",
    "            label_1 = self.csv.iloc[index].grapheme_root\n",
    "            label_2 = self.csv.iloc[index].vowel_diacritic\n",
    "            label_3 = self.csv.iloc[index].consonant_diacritic\n",
    "            label = [label_0, label_1, label_2, label_3]\n",
    "            return torch.tensor(image), torch.tensor(image_origin), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='50210' class='' max='50210', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [50210/50210 00:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='50210' class='' max='50210', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [50210/50210 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='50210' class='' max='50210', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [50210/50210 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='50210' class='' max='50210', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [50210/50210 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = read_data(files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.Cutout(max_h_size=cut_size, max_w_size=cut_size, num_holes=1, p=0.7),\n",
    "])\n",
    "transforms_val = albumentations.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d810319b93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBengaliDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_show\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_show\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "df_show = df_train.iloc[:1000]\n",
    "dataset_show = BengaliDataset(df_show, data, list(range(df_show.shape[0])), 'train', 'train', transform=transforms_train)\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "for i in range(2):\n",
    "    f, axarr = plt.subplots(1,5)\n",
    "    for p in range(5):\n",
    "        idx = np.random.randint(0, len(dataset_show))\n",
    "        img, img_org, label = dataset_show[idx]\n",
    "        axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())\n",
    "        axarr[p].set_title(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = {f'efficientnet-b{i}': f for i, f in enumerate(sorted(glob.glob('bengali/efficientnet-pytorch/*pth')))}\n",
    "\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "class Swish(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "swish = Swish.apply\n",
    "\n",
    "class Swish_module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return swish(x)\n",
    "\n",
    "swish_layer = Swish_module()\n",
    "\n",
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return swish_layer(x)\n",
    "\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    def forward(self, x, target, reduction='mean'):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        if reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif reduction == 'none':\n",
    "            return loss\n",
    "\n",
    "\n",
    "class ArcFaceLoss(nn.modules.Module):\n",
    "    def __init__(self, s=30.0, m=0.5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)             #  0.87758\n",
    "        self.sin_m = math.sin(m)             #  0.47943\n",
    "        self.th = math.cos(math.pi - m)      # -0.87758\n",
    "        self.mm = math.sin(math.pi - m) * m  #  0.23971\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.float()  # float16 to float32 (if used float16)\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))  # equals to **2\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
    "        output *= self.s\n",
    "        loss = DenseCrossEntropy()(output, labels, self.reduction)\n",
    "        return loss / 2\n",
    "\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine\n",
    "\n",
    "\n",
    "class enet_arcface_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone, out_dim_1, out_dim_2):\n",
    "        super(enet_arcface_v2, self).__init__()\n",
    "#         self.enet = enet.EfficientNet.from_name(backbone)\n",
    "#         self.enet.load_state_dict(torch.load(pretrained_dict[backbone]), strict=True)\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        self.enet = nn.Sequential(*list(model.children())[:-1])#chop off last layer\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(0.5) for _ in range(5)\n",
    "        ])\n",
    "\n",
    "#         self.gfc = nn.Linear(self.enet._fc.in_features, 4096)\n",
    "        self.gfc = nn.Linear(2048, 4096)\n",
    "        self.metric_classify = ArcMarginProduct(4096, out_dim_1)\n",
    "        self.myfc_1 = nn.Linear(4096, out_dim_1)\n",
    "        self.myfc_2_1 = nn.Linear(4096, 512)\n",
    "        self.myfc_2_2 = nn.Linear(512, out_dim_2)\n",
    "#         self.enet._fc = nn.Identity()\n",
    "\n",
    "    def extract(self, x):\n",
    "        return self.enet(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        x = x.squeeze()\n",
    "        x = Swish_module()(self.gfc(x))\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                out_1 = self.myfc_1(dropout(x))\n",
    "                out_2 = self.myfc_2_1(dropout(x))\n",
    "            else:\n",
    "                out_1 += self.myfc_1(dropout(x))\n",
    "                out_2 += self.myfc_2_1(dropout(x))\n",
    "        out_1 /= len(self.dropouts)\n",
    "        out_2 /= len(self.dropouts)\n",
    "        out_2 = self.myfc_2_2(Swish_module()(out_2))\n",
    "        metric_output = self.metric_classify(x)\n",
    "        return out_1, out_2, metric_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(logits_1, logits_2, metric_logits, target, loss_weight=loss_weight, is_val=False):\n",
    "\n",
    "    loss_1 = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], target[:, 1]) * loss_weight[1]\n",
    "    loss_2 = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], target[:, 2]) * loss_weight[2]\n",
    "    loss_3 = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], target[:, 3]) * loss_weight[3]\n",
    "\n",
    "    if is_val:\n",
    "        loss = (loss_1 + loss_2 + loss_3) / sum(loss_weight[1:4])\n",
    "    else:\n",
    "        loss_metric = ArcFaceLoss()(metric_logits, F.one_hot(target[:, 0], c0_dim).float()) * loss_weight[4]\n",
    "\n",
    "        loss_0 = nn.CrossEntropyLoss()(logits_1, target[:, 0]) * loss_weight[0]\n",
    "        loss = (loss_0 + loss_1 + loss_2 + loss_3 + loss_metric) / sum(loss_weight)\n",
    "    return loss\n",
    "\n",
    "CE = nn.CrossEntropyLoss()\n",
    "def criterion_mix(logits_1, logits_2, metric_logits, target, loss_weight=loss_weight):\n",
    "    target, shuffled_target, lam = target\n",
    "    \n",
    "    loss_0 = nn.CrossEntropyLoss()(logits_1, target[:, 0]) * loss_weight[0]\n",
    "    loss_1 = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], target[:, 1]) * loss_weight[1]\n",
    "    loss_2 = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], target[:, 2]) * loss_weight[2]\n",
    "    loss_3 = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], target[:, 3]) * loss_weight[3]\n",
    "    loss_metric = ArcFaceLoss()(metric_logits, F.one_hot(target[:, 0], c0_dim).float()) * loss_weight[4]\n",
    "    \n",
    "    loss = (loss_0 + loss_1 + loss_2 + loss_3 + loss_metric) / sum(loss_weight)\n",
    "    \n",
    "    loss_0_mix = nn.CrossEntropyLoss()(logits_1, shuffled_target[:, 0]) * loss_weight[0]\n",
    "    loss_1_mix = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], shuffled_target[:, 1]) * loss_weight[1]\n",
    "    loss_2_mix = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], shuffled_target[:, 2]) * loss_weight[2]\n",
    "    loss_3_mix = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], shuffled_target[:, 3]) * loss_weight[3]\n",
    "    loss_metric_mix = ArcFaceLoss()(metric_logits, F.one_hot(shuffled_target[:, 0], c0_dim).float()) * loss_weight[4]\n",
    "    \n",
    "    loss_mix = (loss_0_mix + loss_1_mix + loss_2_mix + loss_3_mix + loss_metric_mix) / sum(loss_weight)\n",
    "    \n",
    "    return lam * loss + (1 - lam) * loss_mix\n",
    "\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "    \n",
    "def cutmix(data, target, alpha, clip=[0.3, 0.7]):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha), clip[0], clip[1])\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    targets = (target, shuffled_target, lam)\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(solution, submission):\n",
    "\n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[component].values\n",
    "        y_pred_subset = submission[component].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    final_score = np.average(scores, weights=[2,1,1])\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def train_epoch(loader, optimizer, mb):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for (data, data_org, target) in progress_bar(loader, parent=mb):\n",
    "\n",
    "        data, data_org, target = data.to(device), data_org.to(device), target.to(device)\n",
    "        ### mixup & cutmix & cutout\n",
    "        rand_p = np.random.rand()\n",
    "        if rand_p <= 0.0:\n",
    "            data, target = mixup(data_org, target, 1.)  # process from origin\n",
    "            loss_func = criterion_mix\n",
    "        elif 0.0 < rand_p <= 0.0:\n",
    "            data, target = cutmix(data_org, target, 1.)  # process from origin\n",
    "            loss_func = criterion_mix\n",
    "        else:\n",
    "            loss_func = criterion\n",
    "        ###\n",
    "        optimizer.zero_grad()\n",
    "        logits_1, logits_2, metric_logits = model(data)\n",
    "        loss = loss_func(logits_1, logits_2, metric_logits, target)\n",
    "\n",
    "        if not use_amp:\n",
    "            loss.backward()\n",
    "        else:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-20:]) / min(len(train_loss), 20)\n",
    "#         bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "        mb.child.comment = 'loss: %.5f, smth: %.5f' % (loss_np, smooth_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(loader, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    outputs = []\n",
    "    LOGITS_1, LOGITS_2, LOGITS_M = [], [], []\n",
    "    p1, p2, p3 = [], [], []\n",
    "    masks = []\n",
    "    acc, acc1, acc2, acc3 = 0.0,0.0,0.0,0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, data_org, target) in progress_bar(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits_1, logits_2, metric_logits = model(data)\n",
    "\n",
    "            loss = criterion(logits_1, logits_2, metric_logits, target, is_val=True)\n",
    "\n",
    "            pred = logits_1.argmax(1).detach()\n",
    "            pred1 = logits_2[:, :c1_dim].argmax(1).detach()\n",
    "            pred2 = logits_2[:, c1_dim:c1_dim+c2_dim].argmax(1).detach()\n",
    "            pred3 = logits_2[:, c1_dim+c2_dim:].argmax(1).detach()\n",
    "            outputs.append(pred)\n",
    "            p1.append(pred1)\n",
    "            p2.append(pred2)\n",
    "            p3.append(pred3)\n",
    "\n",
    "            acc += (target[:, 0] == pred).sum().cpu().numpy()\n",
    "            acc1 += (target[:, 1] == pred1).sum().cpu().numpy()\n",
    "            acc2 += (target[:, 2] == pred2).sum().cpu().numpy()\n",
    "            acc3 += (target[:, 3] == pred3).sum().cpu().numpy()\n",
    "            \n",
    "            if get_output:\n",
    "                LOGITS_1.append(logits_1)\n",
    "                LOGITS_2.append(logits_2)\n",
    "            LOGITS_M.append(metric_logits)\n",
    "\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        acc = acc / len(dataset_valid) * 100\n",
    "        acc1 = acc1 / len(dataset_valid) * 100\n",
    "        acc2 = acc2 / len(dataset_valid) * 100\n",
    "        acc3 = acc3 / len(dataset_valid) * 100\n",
    "\n",
    "    preds = torch.cat(outputs).cpu().numpy()\n",
    "    solution = df_train.iloc[valid_idx]\n",
    "    submission1 = df_label_map.iloc[preds]\n",
    "    score1 = get_score(solution, submission1)\n",
    "    \n",
    "    submission2 = pd.DataFrame({\n",
    "        'grapheme_root': torch.cat(p1).cpu().numpy(),\n",
    "        'vowel_diacritic': torch.cat(p2).cpu().numpy(),\n",
    "        'consonant_diacritic': torch.cat(p3).cpu().numpy(),\n",
    "    })\n",
    "    score2 = get_score(solution, submission2)\n",
    "\n",
    "    LOGITS_M = torch.cat(LOGITS_M).cpu().numpy()\n",
    "\n",
    "    max_p = LOGITS_M.max(1)\n",
    "    seen_class_acc = 1 - (targets0[np.where(max_p > 0.9)[0]] >= c0_dim).mean()\n",
    "    try:\n",
    "        arcface_recall = np.where((max_p > 0.9) * (targets0 < c0_dim))[0].shape[0] / np.where(targets0 < c0_dim)[0].shape[0]\n",
    "    except:\n",
    "        arcface_recall = 0\n",
    "    if get_output:\n",
    "        LOGITS_1 = torch.cat(LOGITS_1).cpu().numpy()\n",
    "        LOGITS_2 = torch.cat(LOGITS_2).cpu().numpy()\n",
    "        return LOGITS_1, LOGITS_2, LOGITS_M\n",
    "    else:\n",
    "        return val_loss, acc, acc1, acc2, acc3, score1, score2, seen_class_acc, arcface_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "record = [{'train_loss': [], 'val_loss': [], 'score1': [], 'score2': []} for x in range(n_fold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fold = fold\n",
    "train_idx, valid_idx = np.where((df_train['fold'] != i_fold))[0], np.where((df_train['fold'] == i_fold))[0]\n",
    "\n",
    "dataset_train = BengaliDataset(df_train, data, train_idx, 'train', 'train', transform=transforms_train)\n",
    "dataset_valid = BengaliDataset(df_train, data, valid_idx, 'train', 'val', transform=transforms_val)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=RandomSampler(dataset_train), num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, sampler=None, num_workers=num_workers)\n",
    "\n",
    "targets0 = df_train.loc[valid_idx]['grapheme_id'].values\n",
    "\n",
    "model = enet_arcface_v2(enet_type, out_dim_1=c0_dim, out_dim_2=c1_dim+c2_dim+c3_dim)\n",
    "model = model.to(device)\n",
    "# max_score = 0\n",
    "# model_file = f'{kernel_type}_best_fold{i_fold}.pth'\n",
    "\n",
    "# print('Training All Layers...')\n",
    "# optimizer = optim.Adam(model.parameters(), lr=init_lr)\n",
    "# if use_amp:\n",
    "#     model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "# scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "# mb = master_bar(range(1, n_epochs+1))\n",
    "# for epoch in mb:\n",
    "#     print(time.ctime(), 'Epoch:', epoch)\n",
    "#     scheduler_cosine.step(epoch-1)\n",
    "\n",
    "#     train_loss = train_epoch(train_loader, optimizer, mb)\n",
    "#     val_loss, acc, acc1, acc2, acc3, score1, score2, seen_class_acc, arcface_recall = val_epoch(valid_loader)\n",
    "\n",
    "#     content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, val loss: {np.mean(val_loss):.5f}, acc: {(acc):.5f}, acc1: {(acc1):.5f}, acc2: {(acc2):.5f}, acc3: {(acc3):.5f}, seen_class_acc: {(seen_class_acc):.5f}, arcface_recall: {(arcface_recall):.5f} score1: {(score1):.6f}, score2: {(score2):.6f}'\n",
    "#     print(content)\n",
    "#     with open(f'log_{kernel_type}.txt', 'a') as appender:\n",
    "#         appender.write(content + '\\n')\n",
    "\n",
    "#     if score1 >= max_score:\n",
    "#         print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(max_score, score1))\n",
    "#         torch.save(model.state_dict(), model_file)\n",
    "#         max_score = score1\n",
    "\n",
    "#     record[i_fold]['train_loss'].append(np.mean(train_loss))\n",
    "#     record[i_fold]['val_loss'].append(val_loss)\n",
    "#     record[i_fold]['score1'].append(np.mean(score1))\n",
    "#     record[i_fold]['score2'].append(score2)\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(f'{kernel_type}_model_fold{i_fold}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('resnet50-seen_best_fold0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (data, data_org, target) in valid_loader:\n",
    "    data, data_org, target = data.to(device), data_org.to(device), target.to(device)\n",
    "    break\n",
    "data.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ReLU\n",
    "\n",
    "# class GuidedBackprop():\n",
    "#     \"\"\"\n",
    "#        Produces gradients generated with guided back propagation from the given image\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "#         self.gradients = None\n",
    "#         self.forward_relu_outputs = []\n",
    "#         # Put model in evaluation mode\n",
    "#         self.model.eval()\n",
    "#         self.update_relus()\n",
    "#         self.hook_layers()\n",
    "\n",
    "#     def hook_layers(self):\n",
    "#         def hook_function(module, grad_in, grad_out):\n",
    "#             self.gradients = grad_in[0]\n",
    "#         # Register hook to the first layer\n",
    "#         first_layer = list(self.model.enet._modules.items())[0][1]\n",
    "#         first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "#     def update_relus(self):\n",
    "#         \"\"\"\n",
    "#             Updates relu activation functions so that\n",
    "#                 1- stores output in forward pass\n",
    "#                 2- imputes zero for gradient values that are less than zero\n",
    "#         \"\"\"\n",
    "#         def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "#             \"\"\"\n",
    "#             If there is a negative gradient, change it to zero\n",
    "#             \"\"\"\n",
    "#             # Get last forward output\n",
    "#             corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "#             corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "#             modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "#             del self.forward_relu_outputs[-1]  # Remove last forward output\n",
    "#             return (modified_grad_out,)\n",
    "\n",
    "#         def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "#             \"\"\"\n",
    "#             Store results of forward pass\n",
    "#             \"\"\"\n",
    "#             self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "#         # Loop through layers, hook up ReLUs\n",
    "#         for pos, module in self.model.enet._modules.items():\n",
    "#             if isinstance(module, ReLU):\n",
    "#                 module.register_backward_hook(relu_backward_hook_function)\n",
    "#                 module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "#     def generate_gradients(self, input_image, target_class):\n",
    "#         # Forward pass\n",
    "#         logits_1, logits_2, _ = self.model(input_image)\n",
    "#         # Zero gradients\n",
    "#         self.model.zero_grad()\n",
    "#         # Target for backprop\n",
    "#         one_hot_output = torch.FloatTensor(1, logits_2.size()[-1]).zero_().cuda()\n",
    "#         one_hot_output[0][3] = 1\n",
    "#         # Backward pass\n",
    "#         logits_2.backward(gradient=one_hot_output)\n",
    "#         # Convert Pytorch variable to numpy array\n",
    "#         # [0] to get rid of the first channel (1,3,224,224)\n",
    "#         gradients_as_arr = self.gradients.data.cpu().numpy()[0]\n",
    "#         return gradients_as_arr\n",
    "\n",
    "class GuidedBackprop():\n",
    "    \"\"\"\n",
    "       Produces gradients generated with guided back propagation from the given image\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.forward_relu_outputs = []\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "        # Register hook to the first layer\n",
    "        first_layer = list(self.model.enet._modules.items())[0][1]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "    def update_relus(self):\n",
    "        \"\"\"\n",
    "            Updates relu activation functions so that\n",
    "                1- stores output in forward pass\n",
    "                2- imputes zero for gradient values that are less than zero\n",
    "        \"\"\"\n",
    "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "            \"\"\"\n",
    "            If there is a negative gradient, change it to zero\n",
    "            \"\"\"\n",
    "            # Get last forward output\n",
    "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
    "            return (modified_grad_out,)\n",
    "\n",
    "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "            \"\"\"\n",
    "            Store results of forward pass\n",
    "            \"\"\"\n",
    "            self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "        # Loop through layers, hook up ReLUs\n",
    "        for pos, module in self.model.enet._modules.items():\n",
    "            if isinstance(module, ReLU):\n",
    "                module.register_backward_hook(relu_backward_hook_function)\n",
    "                module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class, i):\n",
    "        # Forward pass\n",
    "        logits_1, logits_2, _ = self.model(input_image)\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.FloatTensor(logits_2.size()).zero_().cuda()\n",
    "        one_hot_output[i][target_class[i][1]] = 1\n",
    "        one_hot_output[i][target_class[i][2]+168] = 1\n",
    "        one_hot_output[i][target_class[i][3]+168+11] = 1\n",
    "        # Backward pass\n",
    "        logits_2.backward(gradient=one_hot_output)\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        # [0] to get rid of the first channel (1,3,224,224)\n",
    "        gradients_as_arr = self.gradients.data.cpu().numpy()[i]\n",
    "        return gradients_as_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBP = GuidedBackprop(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-bdc2a3850f64>:14: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  i = ctx.saved_variables[0]\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "guided_grads = GBP.generate_gradients(data, target, i)\n",
    "def save_gradient_images(gradient, file_name):\n",
    "    \"\"\"\n",
    "        Exports the original gradient image\n",
    "    Args:\n",
    "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
    "        file_name (str): File name to be exported\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    # Save image\n",
    "    path_to_file = os.path.join('results/', file_name + '.jpg')\n",
    "    save_image(gradient, path_to_file)\n",
    "    \n",
    "def save_image(im, path):\n",
    "    \"\"\"\n",
    "        Saves a numpy matrix or PIL image as an image\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
    "        path (str): Path to the image\n",
    "    \"\"\"\n",
    "    if isinstance(im, (np.ndarray, np.generic)):\n",
    "        im = format_np_output(im)\n",
    "        im = Image.fromarray(im)\n",
    "    im.save(path)\n",
    "    \n",
    "def format_np_output(np_arr):\n",
    "    \"\"\"\n",
    "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
    "        It converts all the outputs to the same format which is 3xWxH\n",
    "        with using sucecssive if clauses.\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
    "    \"\"\"\n",
    "    # Phase/Case 1: The np arr only has 2 dimensions\n",
    "    # Result: Add a dimension at the beginning\n",
    "    if len(np_arr.shape) == 2:\n",
    "        np_arr = np.expand_dims(np_arr, axis=0)\n",
    "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
    "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
    "    if np_arr.shape[0] == 1:\n",
    "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
    "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
    "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
    "    if np_arr.shape[0] == 3:\n",
    "        np_arr = np_arr.transpose(1, 2, 0)\n",
    "    # Phase/Case 4: NP arr is normalized between 0-1\n",
    "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
    "    if np.max(np_arr) <= 1:\n",
    "        np_arr = (np_arr*255).astype(np.uint8)\n",
    "    return np_arr\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_grayscale(im_as_arr):\n",
    "    \"\"\"\n",
    "        Converts 3d image to grayscale\n",
    "    Args:\n",
    "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
    "    returns:\n",
    "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
    "    \"\"\"\n",
    "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
    "    im_max = np.percentile(grayscale_im, 99)\n",
    "    im_min = np.min(grayscale_im)\n",
    "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
    "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
    "    return grayscale_im\n",
    "\n",
    "def get_positive_negative_saliency(gradient):\n",
    "    \"\"\"\n",
    "        Generates positive and negative saliency maps based on the gradient\n",
    "    Args:\n",
    "        gradient (numpy arr): Gradient of the operation to visualize\n",
    "    returns:\n",
    "        pos_saliency ( )\n",
    "    \"\"\"\n",
    "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
    "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
    "    return pos_saliency, neg_saliency\n",
    "\n",
    "save_gradient_images(data[i].cpu().detach().numpy(), f'{i}_original')\n",
    "save_gradient_images(guided_grads, f'{i}_back_prob_color')\n",
    "save_gradient_images(convert_to_grayscale(guided_grads), f'{i}_back_prob_grey')\n",
    "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
    "save_gradient_images(pos_sal, f'{i}_pos_sal')\n",
    "save_gradient_images(neg_sal, f'{i}_neg_sal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
